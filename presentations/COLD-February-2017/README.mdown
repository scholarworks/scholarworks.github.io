# COLD Presentation Outline
*February 10, 2017 - 10:00am - California State University, San Marcos*

## HISTORY
### 2013 STIM Report
### 2015 STIM Report & Survey
## GROWTH, FUTURE & COSTS
***
### Space & Storage
1. What challenges face us today?

**Narrative:**

```
As we start into 2017 we face several challenges within the scholarworks infrastructure.

Notably: 

- Storage is quickly becoming a liability
- Our average growth rate is expanding

```

**Slide Image**

(http://media.istockphoto.com/photos/loading-new-year-2017-on-chalkboard-background-picture-id622300962)
***
2. What will it cost to approach those challenges without a migration

**Narrative:**
```
- In many ways, not migrating is not an option.
- The CO data center is downsizing away from supporting data heavy services.
- Acquisition of sufficient storage appliances for both today and future growth planning represents a significant financial investment on the front end.
```
**Slide Image**

(http://media.istockphoto.com/photos/finance-conceptual-idea-light-bulb-and-gear-working-on-blackboard-picture-id486455172)
***
3. How will we greatly improve our position in the face of that challenge by moving to the cloud

**Narrative:**

```
- Moving to the cloud offloads the financial investment required for growth to when it's needed.
```

**Slide Image**

(http://media.istockphoto.com/photos/old-way-new-way-on-blackboard-picture-id497127816)

### Performance & Scaling
1. Why is our current performance unsustainable?
**Narrative**
```
- Not to get too technical, but the resourced intensive java stack requires a significant resource investment 
even on a baseline, low-usage system.
- Growth in data also means growth in usage. A shared, resource intense service means that we are forced to invest over
and above the heaviest usage. This is often extreme over committment for lower usage services. While the static nature of the traditional
server platform (often refered to as "bare metal") is vertically scalable, any maintenance requires significan downtime. Downlward 
scalability during low times of usuage for both power can cost savings is impossible.

```
2. What amazing things does the cloud offer us?
**Narrative**
```
The distributed nature of the cloud means that we can levarage any form of mobility across are systems in order
to cost optimiize our services against usage. Not only is storage scalability possible (as it is in a bare metal environment)
it's resource scalable, a vast improvement in the way systems are managed. But not only that, it requires zero downtime and is automated.
```
3. How will we leverage the cloud to address #1.
**Narrative**
```
Distributing the discovery layer by campus (or even project) means that resource allocation can be fine tuned to individual needs. While centralizing the search index and archival repository services allows for a more robust disaster recovery strategy.
```

### Archive, Backup and Recovery
1. How does out current growth projection limit our recovery options
**Narrative**
```
In a traditional server structure, a recovery mirror means duplicate costs. So any data integrity events require traditional tape backup recovery options which mean extended downtime. 
```
2. How does the cloud provide us with a sustainable archival strategy
**Narrative**
```
While we do utilize glacier for long term storage of individual bitstream data, the index, system, etc are all backed up in the typical 
methods provided by the data center. In the cloud, we can mirror every part of the system, to a different geographical region, in a standby mode 
that both minimized cost and provides near zero-downtime recovery capacity.
```
3. .... Profit?

## TIMELINE

### Progress to this point
1. Migration & Feature development
2. Data transition timeline
3. ... Profit?

### Sandbox Timeline

### Deployment Timeline

## Demos

### Working "out-of-the-box" IR w/ some content
1. Bravado demo repository
2. Campus dev repositories

### An Image Repository
1. TODO: Comparing a current CONTENTdm System

### Author / Faculty / Profile Records
1. TODO

### Geospatial Repository
1. TODO: Comparing a map repository (possibly malt in fresno)

### Statistics Dashboard
1. TODO: Import more data

## System needs & Governance
1. More involvement in the ScholarWorks group - Proposal to expand the planning group

### Local Staffing
1. Expectation of local SW members to be liaisons for repository use and needs per campus

### Metadata Best Practices
1. Coordinate with ULMS discovery group on metadata needs
2. Metadata cleanup project to coordinate data normalization